# DLLM Research Papers by Category

## 1. Core Diffusion Models (10 papers)
- [001] Simplified and Generalized Masked Diffusion for Discrete Data
- [005] Why Masking Diffusion Works: Condition on the Jump Schedule for Improved Discrete Diffusion
- [016] Masked Diffusion Models are Secretly Time-Agnostic Masked Models and Exploit Inaccurate Categorical Sampling
- [022] Simple and Effective Masked Diffusion Language Models
- [025] Information-Theoretic Discrete Diffusion
- [028] Scaling up Masked Diffusion Models on Text
- [039] Structured Denoising Diffusion Models in Discrete State-Spaces
- [046] Large Language Diffusion Models (LLaDA)
- [049] A Cheaper and Better Diffusion Language Model with Soft-Masked Noise
- [063] DiffusionBERT: Improving Generative Masked Language Models with Diffusion Models
- [064] Discrete Diffusion Models for Language Generation

## 2. Applications: Code Generation (6 papers)
- [009] Exploring the Power of Diffusion Large Language Models for Software Engineering
- [010] Dream-Coder 7B: An Open Diffusion Language Model for Code
- [013] DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas
- [042] Efficient Training of Language Models to Fill in the Middle
- [052] Beyond Autoregression: An Empirical Study of Diffusion Large Language Models for Code Generation
- [057] DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation

## 3. Training Methods & Alignment (10 papers)
- [002] Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study
- [003] DPO Meets PPO: Reinforced Token Optimization for RLHF
- [004] ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models
- [008] Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks for NLPO
- [015] Pretrain Value, Not Reward: Decoupled Value Policy Optimization
- [032] Optimizing Safe and Aligned Language Generation: A Multi-Objective GRPO Approach
- [034] LLaDA 1.5: Variance-Reduced Preference Optimization for Large Language Diffusion Models
- [036] VRPO: Rethinking Value Modeling for Robust RL Training under Noisy Supervision
- [041] RRHF: Rank Responses to Align Language Models with Human Feedback
- [067] Don't Forget Your Reward Values: Language Model Alignment via Value-based Calibration

## 4. Sampling & Inference Optimization (10 papers)
- [006] Self-Rewarding Sequential Monte Carlo for Masked Diffusion Language Models
- [007] Likelihood-Based Diffusion Language Models
- [011] Path Planning for Masked Diffusion Model Sampling
- [018] Guided Star-Shaped Masked Diffusion
- [019] Theory-Informed Improvements to Classifier-Free Guidance for Discrete Diffusion Models
- [020] STDD: Spatio-Temporal Dynamics-Driven Token Refinement in Diffusion Language Models
- [031] Remasking Discrete Diffusion Models with Inference-Time Scaling
- [035] Review, Remask, Refine (R3): Process-Guided Block Diffusion for Text Generation
- [040] Latent Refinement Decoding: Enhancing Diffusion-Based Language Models
- [043] Think While You Generate: Discrete Diffusion with Planned Denoising
- [068] Don't Settle Too Early: Self-Reflective Remasking for Diffusion Language Models

## 5. Speed & Acceleration (8 papers)
- [023] Learning to Parallel: Accelerating Diffusion Large Language Models via Learnable Parallel Decoding
- [024] Fast Training of Diffusion Models with Masked Transformers
- [047] No Compute Left Behind: Rethinking Reasoning and Sampling with Masked Diffusion Models
- [050] Accelerating Diffusion Language Model Inference via Efficient KV Caching and Guided Diffusion
- [053] Beyond Autoregression: Fast LLMs via Self-Distillation Through Time
- [056] Breaking AR's Sampling Bottleneck: Provable Acceleration via Diffusion Language Models
- [061] Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion Forcing

## 6. Long Context & Memory (5 papers)
- [017] UltraLLaDA: Scaling the Context Length to 128K for Diffusion Large Language Models
- [020] Token Weighting for Long-Range Language Modeling
- [033] LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs
- [048] DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads
- [065] dKV-Cache: The Cache for Diffusion Language Models
- [066] dLLM-Cache: Accelerating Diffusion Large Language Models with Adaptive Caching
- [026] ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference

## 7. Vision & Multimodal (2 papers)
- [038] LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMs
- [045] LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning
- [051] Adapting LLaMA Decoder to Vision Transformer

## 8. AR vs Diffusion Comparative Studies (8 papers)
- [030] TiDAR: Think in Diffusion, Talk in Autoregression
- [044] Double Descent as a Lens for Sample Efficiency in Autoregressive vs. Discrete Diffusion Models
- [054] Beyond Next-Token Prediction: A Performance Characterization of Diffusion vs. Autoregressive LLMs
- [055] Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models
- [058] Diffusion Beats Autoregressive in Data-Constrained Settings
- [059] Diffusion Language Models are Super Data Learners
- [060] Diffusion Language Models Know the Answer Before Decoding
- [062] Diffusion vs. Autoregressive Language Models: A Text Embedding Perspective

## 9. Scaling Laws & Data Efficiency (5 papers)
- [012] Scaling Behavior of Discrete Diffusion Language Models
- [014] Scaling Data-Constrained Language Models
- [021] Scaling Diffusion Language Models via Adaptation from Autoregressive Models
- [027] Scaling Laws for Neural Language Models
- [029] What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?

## 10. Hybrid/Novel Architectures (4 papers)
- [037] Your Absorbing Discrete Diffusion Secretly Models the Bayesian Posterior

---

## Statistics by Category
- **Core Models**: 10 papers
- **Code Generation**: 6 papers
- **Training/Alignment**: 10 papers
- **Sampling/Inference**: 10 papers
- **Speed/Acceleration**: 8 papers
- **Long Context**: 7 papers
- **Vision/Multimodal**: 3 papers
- **AR vs Diffusion**: 8 papers
- **Scaling Laws**: 5 papers
- **Novel Architectures**: 1 paper
- **Total**: 68 papers
